{"cells":[{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"### 使用torchvision.models.detection模型训练与推理\n[参考branch\"pytorch/vision/tree/temp-tutorial\"](https://github.com/pytorch/vision/tree/temp-tutorial/tutorials)<br>\n##### 1. 分类、检测、分割以及关键点检测的模型模块在 [pytorch/vision/models](https://github.com/pytorch/vision/tree/master/torchvision/models)<br>\n&ensp;本地安装的`torchvision`与官方发布的最新版本有很大不同, 所以本地使用的时候以已经安装的`torchvision`库为标准参照.\n##### 2. 微调、训练模型的参考代码位于 [pytorch/vision/references](https://github.com/pytorch/vision/tree/master/references)<br>\n##### 3. 依赖安装项 `cocoapi, torchvision >= 0.3, pytorch, opencv-python, pillow`\n##### 4. baseline\n(1)models<br>\n`backbone(num_class, outchannels, resnet, fpn), rpn(anchor_generator), predictor(fastrcnn_predictor, maskrcnnpredictor)`<br>\n(2)datasets<br>\n(3)transforms<br>\n&ensp;`torchvision`内的`transforms`提供了检测模型最基本的`transforms`方法，`normalize(inputs), resize(inputs, targets)`, 所以在定义数据集的`transforms`时, 只需给定其他数据增强方法;<br>\n(4)train<br>\n&ensp; optimizer, lr_scheduler<br>\n(5)test/evaluation<br>\n&ensp; 模型训练过程中需要评估训练效果, 为避免重复造轮子浪费时间, 可以直接利用cocoapi的evaluate方法来评估模型在自己的数据集上的表现性能.所以需要将自己的数据集包装成coco数据集的形式.<br>\n&ensp; 而`coco_dataset`主要的特点就是, 标注是一个字典, 主要内容有3个, `{\"catogories\":[]], \"images\":[], \"annotations\":[]}`(具体格式详细参考coco数据集说明文档), 所以, 我们只需要<br>\n&ensp; 将自己的数据集和标注信息包装到这个字典去, 然后再填充到`COCO()`对象的`dataset`属性, 就可以了.详细参考`coco_utils.py`.<br>\n##### `目的是使用高度封装的模块化api高效快速搭建和训练模型`<br>\nAPIs:\n- [models](https://github.com/pytorch/vision/tree/master/torchvision/models)\n- [detection](https://github.com/pytorch/vision/tree/master/torchvision/models/detection)\n- [faster_rcnn.FasterRCNN](https://github.com/pytorch/vision/blob/master/torchvision/models/detection/faster_rcnn.py)\n"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"#### FasterR-CNN\n##### FasterRCNN继承自GeneralizeRCNN, 初始化参数表为\n```python\n(self, backbone, num_classes=None,\n    # transform parameters\n    min_size=800, max_size=1333,\n    image_mean=None, image_std=None,\n    # RPN parameters\n    rpn_anchor_generator=None, rpn_head=None,\n    rpn_pre_nms_top_n_train=2000, rpn_pre_nms_top_n_test=1000,\n    rpn_post_nms_top_n_train=2000, rpn_post_nms_top_n_test=1000,\n    rpn_nms_thresh=0.7,\n    rpn_fg_iou_thresh=0.7, rpn_bg_iou_thresh=0.3,\n    rpn_batch_size_per_image=256, rpn_positive_fraction=0.5,\n    # Box parameters\n    box_roi_pool=None, box_head=None, box_predictor=None,\n    box_score_thresh=0.05, box_nms_thresh=0.5, box_detections_per_img=100,\n    box_fg_iou_thresh=0.5, box_bg_iou_thresh=0.5,\n    box_batch_size_per_image=512, box_positive_fraction=0.25,\n    bbox_reg_weights=None)\n```\n##### FasterRCNN主要由backbone, rpn, roi_pooling, fastrcnn_predictor等组成.<br>\n(1)可以根据需要自由选择cnn作为backbone, 比如resnet, fpn等.backbone参数要明确.<br>\n(2)num_classes参数要明确.<br>\n(3)可以根据实际目标尺寸情况提供anchor尺寸组合参数, 生成rpn网络.rpn网络用于生成候选建议窗口.<br>\n"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"#### 搭建目标检测模型"},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":"import torch\nimport torchvision as tv\nfrom torchvision import models\nfrom torchvision.models.detection import faster_rcnn, mask_rcnn\nfrom torchvision.models.detection.rpn import AnchorGenerator\n\nnum_classes=2\n# faster_rcnn.resnet_fpn_backbone内部将backbone的第1, 第2卷积层冻结，不参与更新\nbackbone = faster_rcnn.resnet_fpn_backbone(backbone_name='resnet50', pretrained=True)\nrpn_anchor_generator = AnchorGenerator(sizes=((32,), (64,), (128,), (256,), (512,),),\n                                   aspect_ratios=((0.5, 1.0, 2.0),)*5 )\nmodel = faster_rcnn.FasterRCNN(backbone=backbone, num_classes=num_classes, min_size=600, max_size=600, rpn_anchor_generator=rpn_anchor_generator)"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"#### 封装数据集\n##### 1. linux wget可以很方便地下载链接指向的文件, win10下可以通过安装`pip install wget`来使用.\n`import wget` <br>\n##### 2. 使用PennFudanPed上手学习."},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# import wget\n# import tarfile\n\n# url = 'https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip'\n# out_fname = \"datasets/PennFudanPed.zip\"\n# file_fname = wget.download(url=url, out=out_fname)\n# # 提取压缩包\n# tar = tarfile.open(out_fname)\n# tar.extractall()\n# tar.close()\n# # 删除下载文件压缩包\n# os.remove(out_fname)"},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":"import os\nimport random\nimport numpy as np\nfrom torch.utils import data\nfrom PIL import Image\n\nimport transforms as T\n\n\nclass PennFudanDataset(torch.utils.data.Dataset):\n    def __init__(self, root='datasets/PennFudanPed', train=True, transforms=None):\n        self.root = root\n        self.transforms = transforms\n        # load all image files, sorting them to\n        # ensure that they are aligned\n        imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n        masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n\n        indices = [i for i in range(len(imgs))]\n        random.shuffle(indices)\n        if train:\n            self.imgs = [imgs[i] for i in indices[:-50]]\n            self.masks = [masks[i] for i in indices[:-50]]\n            if transforms == None:\n                transforms = T.Compose([T.ToTensor(), T.RandomHorizontalFlip(0.5)])\n        else:\n            self.imgs = [imgs[i] for i in indices[-50:]]\n            self.masks = [masks[i] for i in indices[-50:]]\n            if transforms == None:\n                transforms = T.Compose([T.ToTensor()])          \n        self.transforms = transforms\n\n\n    def __getitem__(self, idx):\n        # load images ad masks\n        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n        img = Image.open(img_path).convert(\"RGB\")\n        # note that we haven't converted the mask to RGB,\n        # because each color corresponds to a different instance\n        # with 0 being background\n        mask = Image.open(mask_path)\n\n        mask = np.array(mask)\n        # instances are encoded as different colors\n        obj_ids = np.unique(mask)\n        # first id is the background, so remove it\n        obj_ids = obj_ids[1:]\n\n        # split the color-encoded mask into a set\n        # of binary masks\n        masks = mask == obj_ids[:, None, None]\n\n        # get bounding box coordinates for each mask\n        num_objs = len(obj_ids)\n        boxes = []\n        for i in range(num_objs):\n            pos = np.where(masks[i])\n            xmin = np.min(pos[1])\n            xmax = np.max(pos[1])\n            ymin = np.min(pos[0])\n            ymax = np.max(pos[0])\n            boxes.append([xmin, ymin, xmax, ymax])\n\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        # there is only one class\n        labels = torch.ones((num_objs,), dtype=torch.int64)\n        masks = torch.as_tensor(masks, dtype=torch.uint8)\n\n        image_id = torch.tensor([idx])\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        # target[\"masks\"] = masks\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n\n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.imgs)"},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":"import utils\n\ndata_train = PennFudanDataset(train=True)\ndata_test = PennFudanDataset(train=False)\nprint('data_train num=', len(data_train), '\\nfileds:\\n', data_train[0][1])\n# print('data_test num=', len(data_test), '\\nfileds:\\n', data_test[0][1])\ntrainLoader = data.DataLoader(data_train, batch_size=1, shuffle=True, collate_fn=utils.collate_fn)\ntestLoader = data.DataLoader(data_test, batch_size=1, shuffle=False, collate_fn=utils.collate_fn)\nprint(trainLoader, '\\n', testLoader)"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"#### 训练优化\n##### 1. baseline\n&ensp;(1)实例化数据集;将数据集对象提供给数据加载器`torch.utils.data.DataLoader`;<br>\n&ensp;(2)定义优化器和学习率调度器;<br>\n&ensp;(3)迭代训练更新模型, 每一个epoch评估一次验证集;<br>\n##### 2. reference\n&ensp;(1)基本训练模块参考代码来自`pytorch/vision/references/detection`, 这里需要将相应的源代码(主要`engine.py`)下载到本地, 再进行编写;<br>\n&ensp;(2)demo示例<br>\n```python\nfrom engine import train_one_epoch, evaluate\n\n# ...\nmodel.to(device)\n\n# construct an optimizer\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005,\n                            momentum=0.9, weight_decay=0.0005)\n\n# and a learning rate scheduler which decreases the learning rate by\n# 10x every 3 epochs\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                               step_size=3,\n                                               gamma=0.1)\n\nfor epoch in range(num_epochs):\n    # train for one epoch, printing every 10 iterations\n    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n    # update the learning rate\n    lr_scheduler.step()\n    # evaluate on the test dataset\n    evaluate(model, data_loader_test, device=device)                                            \n```\n&ensp;(3)更多细节, 包括训练过程中保存模型参数, 训练日志记录, loss曲线等, 参考`references/train.py, utils.py`<br>"},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":"from engine import train_one_epoch, evaluate\n\ndevice = torch.device('cuda')\nmodel.to(device)\n\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005,\n                            momentum=0.9, weight_decay=0.0005)\n\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                               step_size=3,\n                                               gamma=0.1)\n\nfor epoch in range(10):\n    train_one_epoch(model, optimizer, trainLoader, device, epoch, print_freq=10)\n    lr_scheduler.step()\n    \n    utils.save_on_master({\n                'model': model.state_dict(),\n                'optimizer': optimizer.state_dict(),\n                'lr_scheduler': lr_scheduler.state_dict(),},\n                os.path.join('checkpoints', 'model_{}.pth'.format(epoch)))\n                   \n    evaluate(model, testLoader, device=device)\n"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}